{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc4bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50da9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Working on Sentiment Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d28443",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983f2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = spark.createDataFrame([\n",
    "    (0, \"Hi I think pyspark is cool \",\"happy\"),\n",
    "    (1, \"All I want is a pyspark cluster\",\"indifferent\"),\n",
    "    (2, \"I finally understand how ML works\",\"fulfilled\"),\n",
    "    (3, \"Yet another sentence about pyspark and ML\",\"indifferent\"),\n",
    "    (4, \"Why didn’t I know about mllib before\",\"sad\"),\n",
    "    (5, \"Yes, I can\",\"happy\")\n",
    "], [\"id\", \"sentence\", \"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac381b3",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0efb695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2503bf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|id |sentence                                 |sentiment  |words                                            |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|0  |Hi I think pyspark is cool               |happy      |[hi, i, think, pyspark, is, cool]                |\n",
      "|1  |All I want is a pyspark cluster          |indifferent|[all, i, want, is, a, pyspark, cluster]          |\n",
      "|2  |I finally understand how ML works        |fulfilled  |[i, finally, understand, how, ml, works]         |\n",
      "|3  |Yet another sentence about pyspark and ML|indifferent|[yet, another, sentence, about, pyspark, and, ml]|\n",
      "|4  |Why didn’t I know about mllib before     |sad        |[why, didn’t, i, know, about, mllib, before]     |\n",
      "|5  |Yes, I can                               |happy      |[yes,, i, can]                                   |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc6a87",
   "metadata": {},
   "source": [
    "#### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735bd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"meaningful_words\")\n",
    "meaningful_df = remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88987351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------------------------------------+\n",
      "|words                                            |meaningful_words                     |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "|[hi, i, think, pyspark, is, cool]                |[hi, think, pyspark, cool]           |\n",
      "|[all, i, want, is, a, pyspark, cluster]          |[want, pyspark, cluster]             |\n",
      "|[i, finally, understand, how, ml, works]         |[finally, understand, ml, works]     |\n",
      "|[yet, another, sentence, about, pyspark, and, ml]|[yet, another, sentence, pyspark, ml]|\n",
      "|[why, didn’t, i, know, about, mllib, before]     |[didn’t, know, mllib]                |\n",
      "|[yes,, i, can]                                   |[yes,]                               |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meaningful_df.select('words', 'meaningful_words').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493bcd2",
   "metadata": {},
   "source": [
    "#### Word to Vector transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54757924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "\n",
    "word2Vec = Word2Vec(inputCol=\"words\", outputCol=\"features\")\n",
    "model = word2Vec.fit(meaningful_df)\n",
    "# saving the Word2Vec model to disk\n",
    "model.write().overwrite().save(\"word2Vec\")\n",
    "model_from_disk = Word2VecModel.load(\"word2Vec\")\n",
    "\n",
    "word2Vec_df = model_from_disk.transform(meaningful_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddd5ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               words|            features|\n",
      "+--------------------+--------------------+\n",
      "|[hi, i, think, py...|[-7.6046834389368...|\n",
      "|[all, i, want, is...|[-6.5183000905173...|\n",
      "|[i, finally, unde...|[-7.6046834389368...|\n",
      "|[yet, another, se...|[0.0,0.0,0.0,0.0,...|\n",
      "|[why, didn’t, i, ...|[-6.5183000905173...|\n",
      "|      [yes,, i, can]|[-0.0015209366877...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2Vec_df.select('words', 'features').show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ad0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VarianceThresholdSelector\n",
    "\n",
    "selector = VarianceThresholdSelector(varianceThreshold=0.0, outputCol=\"selectedFeatures\")\n",
    "result_df = selector.fit(word2Vec_df).transform(word2Vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58345ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|  sentiment|               words|    meaningful_words|            features|    selectedFeatures|\n",
      "+---+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|Hi I think pyspar...|      happy|[hi, i, think, py...|[hi, think, pyspa...|[-7.6046834389368...|[-7.6046834389368...|\n",
      "|  1|All I want is a p...|indifferent|[all, i, want, is...|[want, pyspark, c...|[-6.5183000905173...|[-6.5183000905173...|\n",
      "|  2|I finally underst...|  fulfilled|[i, finally, unde...|[finally, underst...|[-7.6046834389368...|[-7.6046834389368...|\n",
      "|  3|Yet another sente...|indifferent|[yet, another, se...|[yet, another, se...|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "|  4|Why didn’t I know...|        sad|[why, didn’t, i, ...|[didn’t, know, ml...|[-6.5183000905173...|[-6.5183000905173...|\n",
      "|  5|          Yes, I can|      happy|      [yes,, i, can]|              [yes,]|[-0.0015209366877...|[-0.0015209366877...|\n",
      "+---+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60379328",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "550b353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"frequencyFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead1578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------------------------------+-------------------------------------------------+\n",
      "|sentiment  |words                                            |frequencyFeatures                                |\n",
      "+-----------+-------------------------------------------------+-------------------------------------------------+\n",
      "|happy      |[hi, i, think, pyspark, is, cool]                |(20,[1,8,9,15,16],[1.0,1.0,1.0,2.0,1.0])         |\n",
      "|indifferent|[all, i, want, is, a, pyspark, cluster]          |(20,[1,2,7,9,12,16],[1.0,1.0,1.0,1.0,2.0,1.0])   |\n",
      "|fulfilled  |[i, finally, understand, how, ml, works]         |(20,[3,6,7,13,16,17],[1.0,1.0,1.0,1.0,1.0,1.0])  |\n",
      "|indifferent|[yet, another, sentence, about, pyspark, and, ml]|(20,[1,6,11,12,16,17],[1.0,2.0,1.0,1.0,1.0,1.0]) |\n",
      "|sad        |[why, didn’t, i, know, about, mllib, before]     |(20,[0,10,12,15,16,19],[1.0,1.0,1.0,1.0,2.0,1.0])|\n",
      "|happy      |[yes,, i, can]                                   |(20,[2,13,16],[1.0,1.0,1.0])                     |\n",
      "+-----------+-------------------------------------------------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.select('sentiment', 'words', 'frequencyFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b1da431",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"frequencyFeatures\", outputCol=\"featureImportance\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e79605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|frequencyFeatures                                |featureImportance                                                                                                         |\n",
      "+-------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|(20,[1,8,9,15,16],[1.0,1.0,1.0,2.0,1.0])         |(20,[1,8,9,15,16],[0.5596157879354227,1.252762968495368,0.8472978603872037,1.6945957207744073,0.0])                       |\n",
      "|(20,[1,2,7,9,12,16],[1.0,1.0,1.0,1.0,2.0,1.0])   |(20,[1,2,7,9,12,16],[0.5596157879354227,0.8472978603872037,0.8472978603872037,0.8472978603872037,1.1192315758708453,0.0]) |\n",
      "|(20,[3,6,7,13,16,17],[1.0,1.0,1.0,1.0,1.0,1.0])  |(20,[3,6,7,13,16,17],[1.252762968495368,0.8472978603872037,0.8472978603872037,0.8472978603872037,0.0,0.8472978603872037]) |\n",
      "|(20,[1,6,11,12,16,17],[1.0,2.0,1.0,1.0,1.0,1.0]) |(20,[1,6,11,12,16,17],[0.5596157879354227,1.6945957207744073,1.252762968495368,0.5596157879354227,0.0,0.8472978603872037])|\n",
      "|(20,[0,10,12,15,16,19],[1.0,1.0,1.0,1.0,2.0,1.0])|(20,[0,10,12,15,16,19],[1.252762968495368,1.252762968495368,0.5596157879354227,0.8472978603872037,0.0,1.252762968495368]) |\n",
      "|(20,[2,13,16],[1.0,1.0,1.0])                     |(20,[2,13,16],[0.8472978603872037,0.8472978603872037,0.0])                                                                |\n",
      "+-------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.select('frequencyFeatures', 'featureImportance').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb7bd0",
   "metadata": {},
   "source": [
    "#### NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e294ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngram_df = ngram.transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8c571ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------------------------------------------------------------------------+\n",
      "|sentence                                 |ngrams                                                                             |\n",
      "+-----------------------------------------+-----------------------------------------------------------------------------------+\n",
      "|Hi I think pyspark is cool               |[hi i, i think, think pyspark, pyspark is, is cool]                                |\n",
      "|All I want is a pyspark cluster          |[all i, i want, want is, is a, a pyspark, pyspark cluster]                         |\n",
      "|I finally understand how ML works        |[i finally, finally understand, understand how, how ml, ml works]                  |\n",
      "|Yet another sentence about pyspark and ML|[yet another, another sentence, sentence about, about pyspark, pyspark and, and ml]|\n",
      "|Why didn’t I know about mllib before     |[why didn’t, didn’t i, i know, know about, about mllib, mllib before]              |\n",
      "|Yes, I can                               |[yes, i, i can]                                                                    |\n",
      "+-----------------------------------------+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram_df.select('sentence', 'ngrams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdf689",
   "metadata": {},
   "source": [
    "#### Encoding the categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35ed7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(result_df).transform(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1ded43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|  sentiment|categoryIndex|\n",
      "+-----------+-------------+\n",
      "|      happy|          0.0|\n",
      "|indifferent|          1.0|\n",
      "|  fulfilled|          2.0|\n",
      "|indifferent|          1.0|\n",
      "|        sad|          3.0|\n",
      "|      happy|          0.0|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.select('sentiment', 'categoryIndex').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
